{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''importing required lib'''\n",
    "import numpy as np # to deal with numbers and array\n",
    "import pandas as pd #for handling dataframes\n",
    "from sklearn.utils import shuffle #use to shuffle dataframe\n",
    "\n",
    "''' lib required for creating a model  i.e text data into array of matrix'''\n",
    "from sklearn.feature_extraction.text import CountVectorizer  ## BOW Model\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer  ## TFIDF Model\n",
    "# for converting text data into array \n",
    "\n",
    "''' lib required for cleaning of data'''\n",
    "from nltk.corpus import stopwords #containing stopwords\n",
    "from nltk.stem.porter import PorterStemmer # use to fing a core meaning of words \n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import re #used for regular expression operation\n",
    "import os\n",
    "import nltk\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'s', 'whom', 'your', \"you've\", 'most', \"won't\", 'were', 'having', 'with', \"doesn't\", 'there', 'do', 'yourself', 'him', 'it', 'll', 'against', 'more', \"that'll\", 'and', 'that', 'after', 'who', 'hadn', 'had', 'why', 'you', 'about', 'than', 't', 'too', 'between', \"you're\", 'does', 'again', 'are', 'wasn', 'y', 'am', 'shouldn', 'nor', \"should've\", 'by', 'for', 'from', 'been', 'themselves', 'they', 'being', 'them', 'shan', 're', \"mustn't\", 'myself', 'hers', 'or', 'above', 'up', \"you'd\", 'where', 'haven', \"don't\", 'each', 'until', 'wouldn', 'we', 'the', 'not', 'own', 'through', 'other', 'their', 'off', \"isn't\", 'few', 'have', 'how', 'further', 'just', 'our', 'd', 'this', 'her', 'if', 'needn', 'so', 'on', 'when', 'will', \"shouldn't\", 'both', 'ain', \"wasn't\", 'aren', 'a', \"hasn't\", 'during', 'same', 'won', \"mightn't\", \"hadn't\", \"it's\", 'o', 'once', 'ma', 'mustn', 'can', \"couldn't\", 'theirs', 'no', 'couldn', 'here', \"didn't\", \"shan't\", 'into', 'below', \"she's\", 'these', 'what', 'has', 'under', 'all', 'those', 'ours', 'be', \"haven't\", 'herself', 'is', 'of', 'isn', 'my', 'weren', 'because', 'some', 'in', 'only', 'didn', 'yours', 'she', 'out', 'down', 'he', 'before', 'should', 'very', 'as', 'me', 'doing', 'to', 'mightn', 've', 'himself', 'i', 'which', 'such', 'yourselves', 'was', \"you'll\", 'its', 'ourselves', 'but', \"weren't\", 'now', 'm', 'hasn', \"wouldn't\", 'don', 'doesn', 'over', 'did', 'while', \"needn't\", 'at', 'then', 'any', 'an', 'itself', 'his', \"aren't\"}\n"
     ]
    }
   ],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "print(stop_words)\n",
    "new_words = [\"RT\",'rt'] #creating a list of extra stop words\n",
    "stop = stop_words.union(new_words) #adding words into corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Input : The whole dataframe will be send to the function\n",
    "Output : Column with clean tweets tweet will be added\n",
    "Function perform following operation:\n",
    "       1. Clean text from html tags\n",
    "       2. Clean text from punctuations and special characters\n",
    "       3. Retain only non-numeric Latin characters with lenght > 2\n",
    "       4. Remove stopwords from the sentence\n",
    "       5. Apply stemming to all the words in the sentence\n",
    "'''\n",
    "def clean_tweets(dataset):\n",
    "    corpus = []\n",
    "    for i in range(0, dataset.shape[0]):\n",
    "        text=dataset['tweet_text'][i]\n",
    "        #hte line data.shape will return [row size,columnsize ] from which we are taking row size using index 0\n",
    "        \n",
    "        #remove @mentions\n",
    "        rg=re.compile('@[^, ]*\\s*')\n",
    "        text=re.sub(rg,\" \",text)\n",
    "\n",
    "        #remove urls\n",
    "        rg=re.compile('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
    "        text=re.sub(rg,\" \",text)\n",
    "\n",
    "        #Remove punctuations\n",
    "        text = re.sub('[^a-zA-Z0-9]', ' ',text)\n",
    "\n",
    "        #Convert to lowercase\n",
    "        text = text.lower()\n",
    "\n",
    "        #remove tags\n",
    "        text=re.sub(\"&lt;/?.*?&gt;\",\" &lt;&gt; \",text)\n",
    "\n",
    "        # remove special characters and digits\n",
    "        #text=re.sub(\"(\\\\d|\\\\W)+\",\" \",text)\n",
    "        text=re.sub(\"(\\\\W)+\",\" \",text)\n",
    "\n",
    "        ##Convert to list from string\n",
    "        text = text.split()\n",
    "\n",
    "        ##Stemming\n",
    "        ps=PorterStemmer()    #Lemmatisation\n",
    "        lem = WordNetLemmatizer()\n",
    "        text = [lem.lemmatize(word) for word in text if not word in\n",
    "                stop_words and len(word)>2]\n",
    "        text = \" \".join(text)\n",
    "        corpus.append(text)\n",
    "    dataset['preprocessed']=corpus\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn import metrics\n",
    "pickle_model=open(\"linearSvc.pickle\" ,'rb')\n",
    "linear_svc_model=pickle.load(pickle_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_tfidf =open(\"tfidf_model_11452.pickle\" ,'rb')\n",
    "tf_idf_vect=pickle.load(pickle_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>tweet_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>other_useful_information</td>\n",
       "      <td>'592326564110585856'</td>\n",
       "      <td>RT @divyaconnects: Reached #Kathmandu finally!...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>other_useful_information</td>\n",
       "      <td>'592590231519555584'</td>\n",
       "      <td>NepalÃÂs Slowing Economy Set for Freefall Wi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>other_useful_information</td>\n",
       "      <td>'593470045814181888'</td>\n",
       "      <td>Food crisis imminent for 1.4 million ppl in Ka...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>other_useful_information</td>\n",
       "      <td>'593155728128516097'</td>\n",
       "      <td>Canadian phone companies waive fees for calls ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>other_useful_information</td>\n",
       "      <td>'593609036144214016'</td>\n",
       "      <td>RT @XuscaSoleNY: https://t.co/o1RqfCCmKm don't...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      label              tweet_id  \\\n",
       "0  other_useful_information  '592326564110585856'   \n",
       "1  other_useful_information  '592590231519555584'   \n",
       "2  other_useful_information  '593470045814181888'   \n",
       "3  other_useful_information  '593155728128516097'   \n",
       "4  other_useful_information  '593609036144214016'   \n",
       "\n",
       "                                          tweet_text  \n",
       "0  RT @divyaconnects: Reached #Kathmandu finally!...  \n",
       "1  NepalÃÂs Slowing Economy Set for Freefall Wi...  \n",
       "2  Food crisis imminent for 1.4 million ppl in Ka...  \n",
       "3  Canadian phone companies waive fees for calls ...  \n",
       "4  RT @XuscaSoleNY: https://t.co/o1RqfCCmKm don't...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe=pd.read_csv('other_useful_information.csv',encoding = 'unicode_escape')\n",
    "dataframe=dataframe.drop(['Unnamed: 0'],axis=1)\n",
    "dataframe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6231, 3)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>preprocessed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>other_useful_information</td>\n",
       "      <td>'592326564110585856'</td>\n",
       "      <td>RT @divyaconnects: Reached #Kathmandu finally!...</td>\n",
       "      <td>reached kathmandu finally lot indian stranded ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>other_useful_information</td>\n",
       "      <td>'592590231519555584'</td>\n",
       "      <td>NepalÃÂs Slowing Economy Set for Freefall Wi...</td>\n",
       "      <td>nepal slowing economy set freefall without wor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>other_useful_information</td>\n",
       "      <td>'593470045814181888'</td>\n",
       "      <td>Food crisis imminent for 1.4 million ppl in Ka...</td>\n",
       "      <td>food crisis imminent million ppl kathmandu lon...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>other_useful_information</td>\n",
       "      <td>'593155728128516097'</td>\n",
       "      <td>Canadian phone companies waive fees for calls ...</td>\n",
       "      <td>canadian phone company waive fee call nepal gl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>other_useful_information</td>\n",
       "      <td>'593609036144214016'</td>\n",
       "      <td>RT @XuscaSoleNY: https://t.co/o1RqfCCmKm don't...</td>\n",
       "      <td>believe earthquake happen naturally mother nat...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      label              tweet_id  \\\n",
       "0  other_useful_information  '592326564110585856'   \n",
       "1  other_useful_information  '592590231519555584'   \n",
       "2  other_useful_information  '593470045814181888'   \n",
       "3  other_useful_information  '593155728128516097'   \n",
       "4  other_useful_information  '593609036144214016'   \n",
       "\n",
       "                                          tweet_text  \\\n",
       "0  RT @divyaconnects: Reached #Kathmandu finally!...   \n",
       "1  NepalÃÂs Slowing Economy Set for Freefall Wi...   \n",
       "2  Food crisis imminent for 1.4 million ppl in Ka...   \n",
       "3  Canadian phone companies waive fees for calls ...   \n",
       "4  RT @XuscaSoleNY: https://t.co/o1RqfCCmKm don't...   \n",
       "\n",
       "                                        preprocessed  \n",
       "0  reached kathmandu finally lot indian stranded ...  \n",
       "1  nepal slowing economy set freefall without wor...  \n",
       "2  food crisis imminent million ppl kathmandu lon...  \n",
       "3  canadian phone company waive fee call nepal gl...  \n",
       "4  believe earthquake happen naturally mother nat...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe=clean_tweets(dataframe)# function calling for preprocessed tweet\n",
    "dataframe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6231, 4)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['typhoon hagupit wreaks havoc leadin filipino environmental voice silencd clim8 talk climatechange'\n",
      " 'batangas province storm signal high alert vice gov tell rubyph'\n",
      " 'updated 1pm today psws rubyph added area signal negro oriental occidental amp guimaras'\n",
      " 'typhoon hagupit bear philippine bbc news'\n",
      " 'cancelled flight due rubyph naia terminal saturday december 2014 via'\n",
      " 'click open street map helped limit devastation typhoon hagupit'\n",
      " 'sky destiny cablelink cignal rubyph'\n",
      " 'advisory class manila city suspended tomorrow december 2014 level walangpasok sabi woooo'\n",
      " 'daylight brings terrifying view super typhoon aiming philippine'\n",
      " 'please advised class lpu manila suspended tom dec 2014 stay safe dry everyone rubyph walangp']\n",
      "6231\n"
     ]
    }
   ],
   "source": [
    "processed_data=dataframe['preprocessed'].values\n",
    "print(processed_data[1245:1255])\n",
    "print(len(processed_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized_data=tf_idf_vect.transform(processed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<6231x11452 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 49179 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorized_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['infrastructure_and_utilities_damage' 'prevention_or_caution_and_advice'\n",
      " 'prevention_or_caution_and_advice' 'prevention_or_caution_and_advice'\n",
      " 'prevention_or_caution_and_advice' 'infrastructure_and_utilities_damage'\n",
      " 'prevention_or_caution_and_advice' 'prevention_or_caution_and_advice'\n",
      " 'prevention_or_caution_and_advice' 'sympathy_and_emotional_support']\n",
      "6231\n"
     ]
    }
   ],
   "source": [
    "#y_pred=linear_svc_model.predict(vectorized_data)\n",
    "print(y_pred[1245:1255])\n",
    "print(len(y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>preprocessed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>infrastructure_and_utilities_damage</td>\n",
       "      <td>'592326564110585856'</td>\n",
       "      <td>RT @divyaconnects: Reached #Kathmandu finally!...</td>\n",
       "      <td>reached kathmandu finally lot indian stranded ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>donation_needs_or_offers_or_volunteering_services</td>\n",
       "      <td>'592590231519555584'</td>\n",
       "      <td>NepalÃÂs Slowing Economy Set for Freefall Wi...</td>\n",
       "      <td>nepal slowing economy set freefall without wor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>donation_needs_or_offers_or_volunteering_services</td>\n",
       "      <td>'593470045814181888'</td>\n",
       "      <td>Food crisis imminent for 1.4 million ppl in Ka...</td>\n",
       "      <td>food crisis imminent million ppl kathmandu lon...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>donation_needs_or_offers_or_volunteering_services</td>\n",
       "      <td>'593155728128516097'</td>\n",
       "      <td>Canadian phone companies waive fees for calls ...</td>\n",
       "      <td>canadian phone company waive fee call nepal gl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>sympathy_and_emotional_support</td>\n",
       "      <td>'593609036144214016'</td>\n",
       "      <td>RT @XuscaSoleNY: https://t.co/o1RqfCCmKm don't...</td>\n",
       "      <td>believe earthquake happen naturally mother nat...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               label              tweet_id  \\\n",
       "0                infrastructure_and_utilities_damage  '592326564110585856'   \n",
       "1  donation_needs_or_offers_or_volunteering_services  '592590231519555584'   \n",
       "2  donation_needs_or_offers_or_volunteering_services  '593470045814181888'   \n",
       "3  donation_needs_or_offers_or_volunteering_services  '593155728128516097'   \n",
       "4                     sympathy_and_emotional_support  '593609036144214016'   \n",
       "\n",
       "                                          tweet_text  \\\n",
       "0  RT @divyaconnects: Reached #Kathmandu finally!...   \n",
       "1  NepalÃÂs Slowing Economy Set for Freefall Wi...   \n",
       "2  Food crisis imminent for 1.4 million ppl in Ka...   \n",
       "3  Canadian phone companies waive fees for calls ...   \n",
       "4  RT @XuscaSoleNY: https://t.co/o1RqfCCmKm don't...   \n",
       "\n",
       "                                        preprocessed  \n",
       "0  reached kathmandu finally lot indian stranded ...  \n",
       "1  nepal slowing economy set freefall without wor...  \n",
       "2  food crisis imminent million ppl kathmandu lon...  \n",
       "3  canadian phone company waive fee call nepal gl...  \n",
       "4  believe earthquake happen naturally mother nat...  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe['label']=y_pred\n",
    "dataframe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "prevention_or_caution_and_advice                            2716\n",
       "infrastructure_and_utilities_damage                          991\n",
       "donation_needs_or_offers_or_volunteering_services            771\n",
       "affectedPeople_or_injured_or_dead_people_or_DeathReports     698\n",
       "sympathy_and_emotional_support                               494\n",
       "disease_transmission_or_disease_signs_or_symptoms            288\n",
       "missing_trapped_or_found_people                              154\n",
       "treatment                                                    119\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6231, 4)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"dataset6231.pickle\" ,'wb') as f:\n",
    "    pickle.dump(dataframe,f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  WE HAVE CATEGORISED OTHER USEFUL INFORMATION INTO OUR DIFFERENT CATEGORY "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "In next ipython notebook we are going to concat this other useful information dataset of 6231 into main dataset\n",
    "\n",
    "cocatination of all dataset will be done \"concatination of all dataset.iypnb \"\n",
    "\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
