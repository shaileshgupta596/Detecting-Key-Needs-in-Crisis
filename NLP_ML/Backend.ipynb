{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Algorithms used\\nnaivebayes>>>MultinomialNB ,BernoulliNB\\nlinear_model>>>>>Logistic regression ,SGDClassifier, \\nSVM>>>>SVC ,LinearSVC ,NuSVC\\nRandomForest\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''importing required lib'''\n",
    "import numpy as np # to deal with numbers and array\n",
    "import pandas as pd #for handling dataframes\n",
    "from sklearn.utils import shuffle #use to shuffle dataframe\n",
    "\n",
    "''' lib required for creating a model  i.e text data into array of matrix'''\n",
    "from sklearn.feature_extraction.text import CountVectorizer  ## BOW Model\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer  ## TFIDF Model\n",
    "# for converting text data into array \n",
    "\n",
    "''' lib required for cleaning of data'''\n",
    "from nltk.corpus import stopwords #containing stopwords\n",
    "from nltk.stem.porter import PorterStemmer # use to fing a core meaning of words \n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import re #used for regular expression operation\n",
    "import os\n",
    "\n",
    "'''Algorithms used\n",
    "naivebayes>>>MultinomialNB ,BernoulliNB\n",
    "linear_model>>>>>Logistic regression ,SGDClassifier, \n",
    "SVM>>>>SVC ,LinearSVC ,NuSVC\n",
    "RandomForest\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' dataset will be collected form https://www.crisisNLP.com\n",
    "d1=pd.read_csv(\"2015_Nepal.csv\")\n",
    "d2=pd.read_csv(\"2014_India_floods.csv\")\n",
    "d3=pd.read_csv(\"typ.csv\")\n",
    "d4=pd.read_csv(\"pakistan_flood.csv\")\n",
    "d5=pd.read_csv(\"cyclone_plam.csv\")\n",
    "d6=pd.read_csv(\"2014_MERS.csv\")\n",
    "d7=pd.read_csv(\"2014_hurrican.csv\")\n",
    "d8=pd.read_csv(\"2014_ebola.csv\")\n",
    "d9=pd.read_csv(\"2014_chile_earthquake.csv\")\n",
    "d10=pd.read_csv(\"2014_california_earthquake.csv\")\n",
    "d11=pd.read_csv(\"2013_pakistaan_eq_cf.csv\")'''\n",
    "dataframe=pd.read_csv('crisis1.csv',encoding = 'unicode_escape')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataframe=pd.concat([d1,d2,d3,d4,d5,d6,d7,d8,d9,d10,d11],ignore_index=True)\n",
    "#concating all the dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>label</th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>tweet_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>other_useful_information</td>\n",
       "      <td>'592326564110585856'</td>\n",
       "      <td>RT @divyaconnects: Reached #Kathmandu finally!...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>missing_trapped_or_found_people</td>\n",
       "      <td>'592616512642420737'</td>\n",
       "      <td>fears for Foreigners missing in Nepal earthqua...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>injured_or_dead_people</td>\n",
       "      <td>'592686635520827393'</td>\n",
       "      <td>RT @ParisBurned: 3,700 people dead is absolute...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>donation_needs_or_offers_or_volunteering_services</td>\n",
       "      <td>'593301431366635520'</td>\n",
       "      <td>Earthquake in Nepal - Please help Kapil #crowd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>other_useful_information</td>\n",
       "      <td>'592590231519555584'</td>\n",
       "      <td>Nepals Slowing Economy Set for Freefall Witho...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                              label  \\\n",
       "0           0                           other_useful_information   \n",
       "1           1                    missing_trapped_or_found_people   \n",
       "2           2                             injured_or_dead_people   \n",
       "3           3  donation_needs_or_offers_or_volunteering_services   \n",
       "4           4                           other_useful_information   \n",
       "\n",
       "               tweet_id                                         tweet_text  \n",
       "0  '592326564110585856'  RT @divyaconnects: Reached #Kathmandu finally!...  \n",
       "1  '592616512642420737'  fears for Foreigners missing in Nepal earthqua...  \n",
       "2  '592686635520827393'  RT @ParisBurned: 3,700 people dead is absolute...  \n",
       "3  '593301431366635520'  Earthquake in Nepal - Please help Kapil #crowd...  \n",
       "4  '592590231519555584'  Nepals Slowing Economy Set for Freefall Witho...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe.head() #showing first five values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "del dataframe['Unnamed: 0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20624, 3)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe.shape # describing the size of dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['label', 'tweet_id', 'tweet_text'], dtype='object')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe.columns #information about the attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>tweet_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>count</td>\n",
       "      <td>20624</td>\n",
       "      <td>20624</td>\n",
       "      <td>20624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>unique</td>\n",
       "      <td>15</td>\n",
       "      <td>18743</td>\n",
       "      <td>18743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>top</td>\n",
       "      <td>other_useful_information</td>\n",
       "      <td>'383910852254244864'</td>\n",
       "      <td>#earthquake #NZ: Magnitude 2.4, Thursday, Sept...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>freq</td>\n",
       "      <td>6231</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           label              tweet_id  \\\n",
       "count                      20624                 20624   \n",
       "unique                        15                 18743   \n",
       "top     other_useful_information  '383910852254244864'   \n",
       "freq                        6231                     2   \n",
       "\n",
       "                                               tweet_text  \n",
       "count                                               20624  \n",
       "unique                                              18743  \n",
       "top     #earthquake #NZ: Magnitude 2.4, Thursday, Sept...  \n",
       "freq                                                    2  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe.describe() # describe the dataframe internal information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label         0\n",
       "tweet_id      0\n",
       "tweet_text    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe.isnull().sum() #determining whether there is a empty value is present or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['other_useful_information', 'missing_trapped_or_found_people',\n",
       "       'injured_or_dead_people',\n",
       "       'donation_needs_or_offers_or_volunteering_services',\n",
       "       'sympathy_and_emotional_support', 'not_related_or_irrelevant',\n",
       "       'caution_and_advice', 'displaced_people_and_evacuations',\n",
       "       'infrastructure_and_utilities_damage', 'prevention',\n",
       "       'disease_transmission', 'deaths_reports',\n",
       "       'disease_signs_or_symptoms', 'affected_people', 'treatment'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe['label'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "label1={'other_useful_information':1, 'missing_trapped_or_found_people':2,\n",
    "       'injured_or_dead_people':3,\n",
    "       'donation_needs_or_offers_or_volunteering_services':4,\n",
    "       'sympathy_and_emotional_support':5, 'not_related_or_irrelevant':6,\n",
    "       'caution_and_advice':7, 'displaced_people_and_evacuations':8,\n",
    "       'infrastructure_and_utilities_damage':9, 'prevention':10,\n",
    "       'disease_transmission':11, 'deaths_reports':12,\n",
    "       'disease_signs_or_symptoms':13, 'affected_people':14, 'treatment':15}\n",
    "def label(x):\n",
    "    return label1[x];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe[\"numeric_label\"]=dataframe[\"label\"].map(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "other_useful_information                             6231\n",
       "not_related_or_irrelevant                            2758\n",
       "injured_or_dead_people                               2427\n",
       "donation_needs_or_offers_or_volunteering_services    2288\n",
       "sympathy_and_emotional_support                       1875\n",
       "infrastructure_and_utilities_damage                  1336\n",
       "caution_and_advice                                   1029\n",
       "affected_people                                       551\n",
       "displaced_people_and_evacuations                      474\n",
       "treatment                                             370\n",
       "disease_signs_or_symptoms                             346\n",
       "disease_transmission                                  336\n",
       "missing_trapped_or_found_people                       277\n",
       "prevention                                            253\n",
       "deaths_reports                                         73\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe['label'].value_counts() # number of events with there count "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RT @cuteinsan: In this horrible situation in nepal our God  @Gurmeetramrahim  can help pita ji they really need your simpathy #MSGHelpEarth",
      "\n",
      "event extracted>>>\n",
      "sympathy_and_emotional_support\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'tweet with corresponding label'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(dataframe['tweet_text'][5])\n",
    "print(\"event extracted>>>\")\n",
    "print(dataframe['label'][5])\n",
    "'''tweet with corresponding label'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18743"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe['tweet_id'].nunique() #number of unique tweet ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Term frequency inverse document freq before preprocessing\n",
    "#tf_idf_vect = TfidfVectorizer()\n",
    "#tf_idf = tf_idf_vect.fit_transform(dataframe['tweet_text'].values)\n",
    "#tf_idf.shape\n",
    "# it's showing very high value of feature preprocessing is required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfrom nltk.corpus import stopwords #containing stopwords\\nfrom nltk.stem.porter import PorterStemmer # use to fing a core meaning of words \\nfrom nltk.stem.wordnet import WordNetLemmatizer\\nimport re #used for regular expression operation\\nimport os '"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "from nltk.corpus import stopwords #containing stopwords\n",
    "from nltk.stem.porter import PorterStemmer # use to fing a core meaning of words \n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import re #used for regular expression operation\n",
    "import os '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"wouldn't\", 'any', 'further', 'a', 'mightn', 'themselves', \"mightn't\", 'his', 'than', 'as', \"aren't\", 'up', 'o', 'while', 'their', 'by', 'herself', 'the', 've', 'will', 'very', 'down', 'has', 'in', 'her', 'about', 'me', \"you're\", 'ourselves', \"haven't\", 're', 'needn', 'do', 'he', 'but', \"mustn't\", \"you'll\", 'both', 'some', 'nor', 'below', 'yourself', 'out', 'couldn', 'into', \"won't\", 'during', 'just', 'few', \"doesn't\", 'this', 'ma', 'before', 'was', 'is', 'to', 'we', 'once', \"didn't\", \"it's\", 'being', 'until', 'how', 'you', 'what', 'am', 'most', 'of', 'mustn', 'against', \"needn't\", 'it', 'if', 'under', 'own', 'll', 'who', \"shan't\", 'our', 'were', 'didn', \"you've\", 'other', 'i', 'd', 'no', 'y', 'doesn', \"she's\", 'then', 'such', \"that'll\", 'wouldn', \"hasn't\", 'doing', 'and', \"should've\", 'after', \"hadn't\", 'again', 's', 'that', 'only', 'yours', 'are', 'does', 'when', 'been', 'here', 'which', 'hadn', 'on', 'off', \"wasn't\", 'so', \"weren't\", 'did', \"don't\", 'from', 'now', 't', 'those', 'there', 'these', 'have', 'm', 'she', 'an', 'theirs', 'between', 'not', 'its', 'shan', 'over', 'your', \"isn't\", 'each', 'why', 'all', 'having', 'at', 'had', 'them', 'where', 'haven', 'wasn', 'him', 'myself', 'should', 'can', 'for', 'with', 'aren', 'they', \"you'd\", \"couldn't\", 'himself', 'same', 'more', 'isn', 'be', 'don', 'or', 'ours', 'ain', 'because', 'yourselves', 'won', 'itself', 'shouldn', 'whom', 'weren', 'hers', 'too', 'hasn', \"shouldn't\", 'through', 'my', 'above'}\n"
     ]
    }
   ],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_words = [\"RT\",'rt'] #creating a list of extra stop words\n",
    "stop_words = stop_words.union(new_words) #adding words into corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Input : The whole dataframe will be send to the function\n",
    "Output : Column with clean tweets tweet will be added\n",
    "Function perform following operation:\n",
    "       1. Clean text from html tags\n",
    "       2. Clean text from punctuations and special characters\n",
    "       3. Retain only non-numeric Latin characters with lenght > 2\n",
    "       4. Remove stopwords from the sentence\n",
    "       5. Apply stemming to all the words in the sentence\n",
    "'''\n",
    "def clean_tweets(dataset):\n",
    "    corpus = []\n",
    "    for i in range(0, dataset.shape[0]):\n",
    "        text=dataset['tweet_text'][i]\n",
    "        #hte line data.shape will return [row size,columnsize ] from which we are taking row size using index 0\n",
    "        \n",
    "        #remove @mentions\n",
    "        rg=re.compile('@[^, ]*\\s*')\n",
    "        text=re.sub(rg,\" \",text)\n",
    "\n",
    "        #remove urls\n",
    "        rg=re.compile('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
    "        text=re.sub(rg,\" \",text)\n",
    "\n",
    "        #Remove punctuations\n",
    "        text = re.sub('[^a-zA-Z0-9]', ' ',text)\n",
    "\n",
    "        #Convert to lowercase\n",
    "        text = text.lower()\n",
    "\n",
    "        #remove tags\n",
    "        text=re.sub(\"&lt;/?.*?&gt;\",\" &lt;&gt; \",text)\n",
    "\n",
    "        # remove special characters and digits\n",
    "        #text=re.sub(\"(\\\\d|\\\\W)+\",\" \",text)\n",
    "        text=re.sub(\"(\\\\W)+\",\" \",text)\n",
    "\n",
    "        ##Convert to list from string\n",
    "        text = text.split()\n",
    "\n",
    "        ##Stemming\n",
    "        ps=PorterStemmer()    #Lemmatisation\n",
    "        lem = WordNetLemmatizer()\n",
    "        text = [lem.lemmatize(word) for word in text if not word in\n",
    "                stop_words]\n",
    "        text = \" \".join(text)\n",
    "        corpus.append(text)\n",
    "    dataset['preprocessed']=corpus\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>numeric_label</th>\n",
       "      <th>preprocessed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>other_useful_information</td>\n",
       "      <td>'592326564110585856'</td>\n",
       "      <td>RT @divyaconnects: Reached #Kathmandu finally!...</td>\n",
       "      <td>1</td>\n",
       "      <td>reached kathmandu finally lot indian stranded ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>missing_trapped_or_found_people</td>\n",
       "      <td>'592616512642420737'</td>\n",
       "      <td>fears for Foreigners missing in Nepal earthqua...</td>\n",
       "      <td>2</td>\n",
       "      <td>fear foreigner missing nepal earthquake death ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>injured_or_dead_people</td>\n",
       "      <td>'592686635520827393'</td>\n",
       "      <td>RT @ParisBurned: 3,700 people dead is absolute...</td>\n",
       "      <td>3</td>\n",
       "      <td>3 700 people dead absolutely devastating count...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>donation_needs_or_offers_or_volunteering_services</td>\n",
       "      <td>'593301431366635520'</td>\n",
       "      <td>Earthquake in Nepal - Please help Kapil #crowd...</td>\n",
       "      <td>4</td>\n",
       "      <td>earthquake nepal please help kapil crowdfundin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>other_useful_information</td>\n",
       "      <td>'592590231519555584'</td>\n",
       "      <td>Nepals Slowing Economy Set for Freefall Witho...</td>\n",
       "      <td>1</td>\n",
       "      <td>nepal slowing economy set freefall without wor...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               label              tweet_id  \\\n",
       "0                           other_useful_information  '592326564110585856'   \n",
       "1                    missing_trapped_or_found_people  '592616512642420737'   \n",
       "2                             injured_or_dead_people  '592686635520827393'   \n",
       "3  donation_needs_or_offers_or_volunteering_services  '593301431366635520'   \n",
       "4                           other_useful_information  '592590231519555584'   \n",
       "\n",
       "                                          tweet_text  numeric_label  \\\n",
       "0  RT @divyaconnects: Reached #Kathmandu finally!...              1   \n",
       "1  fears for Foreigners missing in Nepal earthqua...              2   \n",
       "2  RT @ParisBurned: 3,700 people dead is absolute...              3   \n",
       "3  Earthquake in Nepal - Please help Kapil #crowd...              4   \n",
       "4  Nepals Slowing Economy Set for Freefall Witho...              1   \n",
       "\n",
       "                                        preprocessed  \n",
       "0  reached kathmandu finally lot indian stranded ...  \n",
       "1  fear foreigner missing nepal earthquake death ...  \n",
       "2  3 700 people dead absolutely devastating count...  \n",
       "3  earthquake nepal please help kapil crowdfundin...  \n",
       "4  nepal slowing economy set freefall without wor...  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe=clean_tweets(dataframe)# function calling for preprocessed tweet\n",
    "dataframe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RT @ParisBurned: 3,700 people dead is absolutely devastating for a country that is the size of Oklahoma on the map. #Nepal\n",
      "After preprocessing>>>>>>>>>>>>>>>>>\n",
      "3 700 people dead absolutely devastating country size oklahoma map nepal\n"
     ]
    }
   ],
   "source": [
    "print(dataframe['tweet_text'][2])\n",
    "print('After preprocessing>>>>>>>>>>>>>>>>>')\n",
    "print(dataframe['preprocessed'][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['reached kathmandu finally lot indian stranded airport nepalquake'\n",
      " 'fear foreigner missing nepal earthquake death toll soar'\n",
      " '3 700 people dead absolutely devastating country size oklahoma map nepal'\n",
      " 'earthquake nepal please help kapil crowdfunding fundrazr support retweet via mechanismofwar'\n",
      " 'nepal slowing economy set freefall without world help via']\n",
      "corresponding result >>>>>>>>>>>>>>>>>>>>\n",
      "[1 2 3 4 1]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "X = dataframe['preprocessed'].values# text data will be converted into operational format\n",
    "y=dataframe['numeric_label'].values #same as it is'''\n",
    "print(X[0:5])\n",
    "print('corresponding result >>>>>>>>>>>>>>>>>>>>')\n",
    "print(y[0:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20624, 18961)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_idf_vect = TfidfVectorizer()# initializing the vector by creating object\n",
    "X= tf_idf_vect.fit_transform(X) # tranforming text data into vector format\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.1, random_state = 123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forest_model=RandomForestClassifier(n_estimators=400, max_depth=5, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "                       max_depth=5, max_features='auto', max_leaf_nodes=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, n_estimators=400,\n",
       "                       n_jobs=None, oob_score=False, random_state=0, verbose=0,\n",
       "                       warm_start=False)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_forest_model.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, ..., 1, 1, 1], dtype=int64)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred=random_forest_model.predict(X_test)#after training we pass rest 20%of data for testing purpose\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acuuracy>>>> 29.762481822588466\n"
     ]
    }
   ],
   "source": [
    "acc=metrics.accuracy_score(y_test,y_pred) \n",
    "#here we are passing actual result and predicted result to find out accuracy\n",
    "print('acuuracy>>>>',acc*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=XGBClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 241.43729829788208 seconds ---\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "start_time=time()\n",
    "model.fit(X_train,y_train)\n",
    "print(\"--- %s seconds ---\" % (time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7, 3, 1, ..., 5, 6, 1], dtype=int64)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred=model.predict(X_test)#after training we pass rest 20%of data for testing purpose\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acuuracy>>>> 69.36500242365487\n"
     ]
    }
   ],
   "source": [
    "acc=metrics.accuracy_score(y_test,y_pred) \n",
    "#here we are passing actual result and predicted result to find out accuracy\n",
    "print('acuuracy>>>>',acc*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import ctime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "starttime=ctime()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Feb 22 17:27:43 2020\n"
     ]
    }
   ],
   "source": [
    "print(starttime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
